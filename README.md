# Prompt Engineering & AI Trainer Portfolio
**Devakrishna Anilkumar**

This repository showcases **real-world prompt engineering, LLM evaluation, and model-alignment experiments**.
The focus is not on clever prompts, but on **evaluating, constraining, and improving model behavior** in ways that reflect real AI Trainer and Evaluator workflows.

The experiments demonstrate how large language models behave under:
- Ambiguous instructions
- Multi-turn refinement
- Role and persona constraints
- Abstract scientific and philosophical queries
- Safety-sensitive and misinformation-prone topics

This portfolio is designed to reflect practical skills required for:
- **AI Trainer**
- **LLM Evaluator**
- **Prompt Engineer (Junior)**
- **AI Quality / Alignment Analyst** roles

---

## Core Skills Demonstrated
- Prompt design and iterative refinement  
- Instruction-following evaluation  
- Hallucination detection and mitigation  
- Ambiguity & metaphor handling  
- Long-context reasoning analysis  
- Audience-aware response evaluation  
- Evidence calibration & verification checklists  
- Safety-aware prompt constraints  

---

## How to Read This Repository
Each experiment is structured to mirror real evaluation tasks and includes:
- Prompt objective  
- Original / ambiguous prompt  
- Common model failure modes  
- Evaluation criteria  
- Improved or constrained prompt  
- Why the experiment matters for AI training  

The emphasis is on **model behavior analysis**, not model output generation.

---

## Experiments

### 1️⃣ Instruction Following
**File:** `01_instruction_following/clarity_vs_ambiguity.md`

Examines how vague prompts lead to inconsistent outputs and how adding
structure, constraints, and scope boundaries improves instruction compliance
and reduces hallucination risk.

---

### 2️⃣ Prompt Refinement (Multi-Turn Reasoning)
**File:** `02_prompt_refinement/multi_turn_reasoning.md`

Demonstrates how outputs improve across turns as prompts evolve from:
- vague → constrained → measurable

This reflects real AI Trainer workflows where prompts are refined through
feedback loops to stabilize quality and evaluability.

---

### 3️⃣ Role / Persona Prompting
**File:** `03_role_persona_prompting/audience_adaptation.md`

Evaluates how well models adapt explanations based on audience:
- layperson vs technical user

Focuses on tone control, vocabulary alignment, and preservation of factual
accuracy across different user contexts.

---

### 4️⃣ Hallucination Detection (Myth vs Evidence)
**File:** `04_hallucination_detection/myth_vs_evidence.md`

Tests model behavior when handling viral or misleading claims.
Emphasizes:
- separation of myth, speculation, and evidence
- avoidance of fabricated sources
- calibrated uncertainty and verification guidance

---

### 5️⃣ Ambiguous Prompts (Metaphor vs Fact)
**File:** `05_ambiguous_prompts/metaphor_vs_fact.md`

Explores how models handle metaphorical language that is often mistaken for
literal scientific claims (e.g., “DNA as an antenna”).

This category is critical for preventing pseudo-science drift while preserving
helpful explanations.

---

## Evaluation Framework
**File:** `evaluation_framework.md`

Defines consistent criteria used across experiments, including:
- Instruction compliance
- Hallucination risk
- Conceptual accuracy
- Uncertainty calibration
- Safety and misinformation handling
- Long-context consistency

This framework mirrors how outputs are scored in professional AI evaluation
pipelines.

---

## Why This Portfolio Matters
Most harmful or misleading AI outputs are not caused by direct falsehoods,
but by:
- ambiguous prompts
- uncalibrated confidence
- misinterpreted metaphors
- poor audience alignment

This repository focuses on **preventing those failure modes**, which is a core
responsibility of AI Trainers and Evaluators in production systems.

---

## Author
**Devakrishna Anilkumar**  
Background: Data Science & AI  
Focus: AI evaluation, alignment, and prompt engineering  

# Prompt Engineering & AI Trainer Portfolio
**Devakrishna Anilkumar**

Real-world prompt engineering, AI evaluation, and model alignment experiments
- Misinformation / myth-vs-evidence evaluation
- Evidence calibration & verification checklists
